---
title: "Aula 09 - PSM"
author: "Fernando Lhamas"
format: html
editor: visual
---

## Propensity Score Matching

A partir de um design observacional, podemos realizar uma técnica de pareamento estatístico, que busca estimar o efeito de um tratamento, política ou intervenção, levando em consideração possíveis **covariáveis** que podem afetar o tratamento.

Rosenbaum e Rubin introduziram está técnica em 1983, atribuindo uma probabilidade condicional de cada observação fazer parte do tratamento. Esta probabilidade de fazer parte do tratamento, é baseada nos scores das covariáveis.

Foi percebido então, que ao adotar este método antes de efetivamente estimar os efeitos de um tratamento em uma população em um estudo observacional, podemos reduzir as ameaças à validade interna, provocadas principalmente pela não randomização, ou seja, por não ter um **design experimental.**

O matching nada mais é do que a criação de um grupo de controle artificial, com probabilidades pareadas entre os grupos par a par (tratamento e controle).

O **score de propensão** indica o quão apto ou propenso, a observação está para receber o tratamento. Esse score é formado justamente pelas covariáveis (características que influenciam a variável de tratamento).

## Vantagens e desvantagens

-   O PSM consegue reduzir substancialmente o viés de seleção em estudos observacionais

-   Não podemos dizer que o design observacional foi alterado para um experimento natural, simplesmente por adotar o PSM na análise. O PSM altera o design, se aproximando de um design quasi-experimental.

-   Apesar do evento ou tratamento ser natural, O PSM é deliberadamente aplicado pelo pesquisador para equilibrar os grupos e fortalecer as inferências causais.

-   O PSM não considera variáveis latentes como covariáveis. Isso quer dizer que, qualquer efeito não observável que esteja confundindo as relações entre as variáveis de interesse, não são aplicáveis no PSM.

-   As alternativas para o caso anterior, incluem modelagem de equações estruturais, análise multinível e outras técnicas de quasi-experimento que lidam com variáveis latentes (variáveis instrumentais e regressão discontínua, por exemplo).

-   O próprio PSM, com algumas inovações no algoritmo, permite trabalhar com variáveis latentes. Normalmente, as técnicas quasi-experimentais tem recebido atenção dos pesquisadores para gerar versões com sufixo latente.

## Exemplo completo

Vamos analisar os dados de crianças (ecls.csv) sobre o efeito de ir em uma escola católica oposta a uma escola pública, para verificar o desempenho em matemática.

Os dados podem ser obtidos aqui

\<<https://github.com/sejdemyr/ecls/tree/master>\>

O estudo pode ser conferido aqui, assim como o codebook

\<<https://www.childandfamilydataarchive.org/cfda/archives/cfda/studies/4075>\>

Vamos seguir o passo-a-passo do PSM para realizar a análise.

Vamos iniciar carregando os pacotes e importando a base:

```{r}
library(MatchIt) # para o PSM
library(tidyverse)
ecls <- read.csv("ecls.csv")
```

## Explorando os dados

Vamos começar gerando algumas saídas referentes a diferença de médias dos grupos na variável resposta.

A variável resposta é a nota estandardizada de matemática ('c5r2mtsc_std'), e a variável de tratamento é dicotômica ('catholic').

```{r}
ecls %>%
  group_by(catholic) %>%
  summarise(n_students = n(),
            mean_math = mean(c5r2mtsc_std),
            std_error = sd(c5r2mtsc_std)/sqrt(n_students))
```

Percebemos que estudantes das escolas católicas tiveram nota de matemática maior do que os de escola pública (em 20% a mais, considerando o desvio padronizado).

Vamos ver o resultados sem valores padronizados

```{r}
ecls %>%
  group_by(catholic) %>%
  summarise(n_students = n(),
            mean_math = mean(c5r2mtsc),
            std_error = sd(c5r2mtsc)/sqrt(n_students))
```

Aparentemente as notas estão na mesma escala, então podemos conduzir um teste de comparação de médias sem os valores padronizados. Mas antes, vamos ver como a nota bruta foi padronizada. Para não alterar a base original, coloquei em um novo objeto `ecls2`, a variável `mat_stand`, que é a mesma de `c5r2mtsc_std`

```{r}
ecls2 <- ecls %>%
  mutate(mat_stand = (c5r2mtsc - mean(c5r2mtsc)) / sd(c5r2mtsc))
```

Agora conduzindo o teste, vamos utilizar o teste t de amostras independentes, já que temos dois grupos independentes na variável de tratamento.

```{r}
t.test(c5r2mtsc_std ~ catholic, data=ecls)
```

Obtivemos uma diferença significativa, com p \< 0,05 e t = -9,10. Assim, evidenciamos a hipótese alternativa, rejeitando a hipótese nula de que as médias de matemática entre os grupos são iguais.

## Avaliando as covariáveis utilizadas

-   `race_white`: Dummy se o estudante é branco (1) ou não (0)
-   `p5hmage`: Idade da mãe
-   `w3income`: Renda familiar
-   `p5numpla`: Número de lugares em que o estudante morou nos últimos 4 meses
-   `w3momed_hsb`: Se a mãe tem ensino médio completo ou abaixo (1) ou ensino superior (0)?

```{r}
ecls_cov <- c('race_white', 'p5hmage', 'w3income', 'p5numpla', 'w3momed_hsb')
ecls %>%
  group_by(catholic) %>%
  select(one_of(ecls_cov)) %>%
  summarise_all(funs(mean(., na.rm = T)))
```

Ao considermos o uso de covariáveis na análise, passamos por um processo em que identificamos na literatura que as covariáveis podem influenciar a relação mensurada pelas variáveis de interesse. Ao não adotar essas covariáveis na análise, podemos tipifica-las como confundidoras. Ao decidir adotar, vamos tipifica-las como variáveis de controle. Quando varremos a literatura, identificamos possíveis covariáveis e decidimos inclui-las na análise, estamos aumentando a validade interna do modelo a ser testado.

Verificamos de forma exploratória, que há diferenças de médias das covariáveis com a variável de tratamento. Vamos verificar se estas diferenças são significativas:

```{r}
lapply(ecls_cov, function(v) {
    t.test(ecls[, v] ~ ecls[, 'catholic'])
})
```

Apontamos que todas as candidatas a variável controle indicam diferenças significativas entre os grupos. Portanto, vale a pena incluir todas como variáveis de controle.

## Estimação dos propensity scores

Agora vamos começar o PSM. Decidimos adotar uma técnica de quasi-experimento que busca criar um grupo de controle artificial, agrupando pares de observações, com características muito parecidas, baseadas nas covariáveis. Dessa forma, podemos isolar os efeitos de outras variáveis na relação que queremos mensurar. Em outras palavras, estamos construindo um contrafactual, alterando o design da pesquisa para um mais robusto (com maior validade interna).

```{r}
ecls <- ecls %>% mutate(w3income_1k = w3income / 1000)
m_ps <- glm(catholic ~ race_white + w3income_1k + p5hmage + p5numpla + w3momed_hsb,
            family = binomial(), data = ecls)
summary(m_ps)
```

Basicamente, rodamos um modelo de regressão logística para calcular os propensity scores (a probabilidade de cada observação de fazer parte de um grupo da variável de tratamento). Com esse passo inicial, observações que são de um grupo, mas tem características comuns a de outro, são classificados com base nas características (covariáveis).

```{r}
prs_df <- data.frame(pr_score = predict(m_ps, type = "response"),
                     catholic = m_ps$model$catholic)
head(prs_df)
```

## Análise da área de suporte comum

Vamos verificar o resultado da classificação de scores, utilizando histogramas:

```{r}
labs <- paste("São de escola:", c("Catholic", "Public"))
prs_df %>%
  mutate(catholic = ifelse(catholic == 1, labs[1], labs[2])) %>%
  ggplot(aes(x = pr_score)) +
  geom_histogram(color = "white") +
  facet_wrap(~catholic) +
  xlab("Probabilidade de ser de escola católica") +
  theme_bw()
```

Como o grupo de escola católica é menor, é natural que os propensity scores acompanhem o número de observações do grupo menor.

## Executando o matching

Agora podemos parear os dados. Estamos limitados a região de suporte comum, como foi definido anteriormente. O método ou algoritmo adotado aqui, irá procurar pares de observações com propensity scores mais próximos. Na limitação deste método, outros podem ser adotados. Neste momento, também vamos omitir os dados ausentes, pois atrapalham o algoritmo de matching.

```{r}
ecls_nomiss <- ecls %>%  # MatchIt does not allow missing values
  select(c5r2mtsc_std, catholic, one_of(ecls_cov)) %>%
  na.omit()

mod_match <- matchit(catholic ~ race_white + w3income + p5hmage + p5numpla + w3momed_hsb,
                     method = "nearest", data = ecls_nomiss)
```

O modelo foi gerado. Agora, podemos verificar alguns resultados de qualidade do modelo:

```{r}
dta_m <- match.data(mod_match)
dim(dta_m)
summary(mod_match)
```

Queremos que as diferenças de médias estema próximas de 0, para assim corroborar o bom pareamento, indicando que temos grupos bem parecidos e, consequentemente, controlando as covariáveis. Conseguimos parear 1352 observações de cad grupo (controle e tratamento), sem descartar nenhum dado não ausente do menor grupo (dos que estudam em escola católica).

Se tivermos problemas em gerar o modelo de matching, ou em obter bom ajustamento, podemos tentar outros algoritmos de matching.

Vamos agora verificar a diferença de médias que obtivemos:

```{r}
dta_m %>%
  group_by(catholic) %>%
  select(one_of(ecls_cov)) %>%
  summarise_all(funs(mean))
```

Uma confirmação ainda mais precisa que o matching funcionou é rodar testes de comparação de médias, buscando encontrar a não rejeição da hipótese nula.

```{r}
lapply(ecls_cov, function(v) {
    t.test(dta_m[, v] ~ dta_m$catholic)
})
```

## Efeito do tratamento na variável resposta

Finalmente, vamos nos voltar para as variáveis de interesse e rodar o modelo estatístico novamente, mas dessa vez, nos dados pareados.

```{r}
t.test(c5r2mtsc_std ~ catholic, data = dta_m)
```

Percebemos que agora a interpretação é o contrário. As notas de matemáticas são melhores na escola pública.

```{r}
dta_m %>%
  group_by(catholic) %>%
  summarise(n_students = n(),
            mean_math = mean(c5r2mtsc_std),
            std_error = sd(c5r2mtsc_std)/sqrt(n_students))
```

Quando isolamos as covariáveis (antes confundindo essa relação), obtivemos 16% a mais de desvios padronizados da nota de matemática para o grupo 0 (estudantes de escola pública).

## Alguns apontamentos

-   Não é sempre que o uso do PSM consegue provocar esta mudança de interpretação das hipóteses, como ocorreu neste exemplo.
-   No entanto, conseguimos obter uma magnitude mais fidedigna, mais próxima da realidade, e assim podemos tomar decisões melhores.
-   Neste caso específico, a conclusão seria completamente equivocada, caso não adotassemos o PSM.
-   O design de pesquisa anterior era inapropriado para avaliar as diferenças no grupo de tratamento, justamente por haver serias ameaças à validade interna.
-   O PSM não só contornou esta situação, mas fez isso a partir de dados já coletados, sem a necessidade de ir a campo novamente fazer um experimento real (provavelmente seria inviável ou iria demorar muito tempo e seria caro).

## Validação com subamostra

Sabemos que um n grande pode inflar o poder estatístico, levando a decisões erradas em testes de hipóteses. Portanto, para validar os resultados, é importante refazer o teste com as variáveis de interesse a partir de subamostra com n controlado (a partir do cálculo de poder estatístico).

Como temos uma larga amostra, vamos ser conservadores nos parâmetros de cálculo de amostra. Cohen define que para o teste t, temos um efeito pequeno em 0,2, médio em 0,5 e grande 0,8.

```{r}
library (pwr)

#Verificando amostra com power = 0,95

pwr_result <- pwr.t.test(d = 0.2, power = 0.95, sig.level = 0.05, type = "two.sample")
total_sample_size <- round(pwr_result$n)
total_sample_size

# Seleção aleatória proporcional por grupo
sub_sample <- dta_m %>%
  group_by(catholic) %>%
  slice_sample(n = total_sample_size) %>%
  ungroup()
```

Agora fazemos o teste novamente com a esperança de obter resultados semelhantes

```{r}
t.test(c5r2mtsc_std ~ catholic, data = sub_sample)
```

Obtivemos regra de decisão igual. Portanto, o efeito inflacionado do poder estatístico não altera a conclusão final: Alunos de escolas públicas tem notas substancialmente maiores do que os alunos de escolas católicas.

```{r}
# Configurar número de repetições
num_repeticoes <- 20  # Número de subamostragens

# Armazenar p-valores
p_valores <- numeric(num_repeticoes)

# Loop para realizar subamostragens e calcular os p-valores
for (i in 1:num_repeticoes) {
  # Gerar subamostra
  sub_sample <- dta_m %>%
    group_by(catholic) %>%
    slice_sample(n = total_sample_size) %>%
    ungroup()
  
  # Realizar teste t com a subamostra
  teste_t <- t.test(c5r2mtsc_std ~ catholic, data = sub_sample)
  p_valores[i] <- teste_t$p.value
}

# Criar um data frame com os resultados
resultados <- data.frame(
  Tentativa = 1:num_repeticoes,
  P_Valor = p_valores
)

# Criar o gráfico
ggplot(resultados, aes(x = Tentativa, y = P_Valor)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_point(color = "blue") +
  geom_line(color = "blue", alpha = 0.7) +
  labs(
    title = "P-Valores entre Subamostras",
    x = "Tentativa (Subsample)",
    y = "Diferença de P-Valor (em relação ao centro)"
  ) +
  theme_minimal()

```

Caso você tenha muitas subamostras com p \> 0,05, um procedimento mais robusto pode ser feito: Ao invés de gerar subamostras da amostra pareada, você pode fazer o pareamento de cada subamostra. Assim, garantimos um melhor balanceamento. Se mesmo na forma mais conservadora de cálculo de amostra, os p-valores da subamostras discordarem, podemos afirmar que a diferença é inconclusiva e portanto, não rejeitamos a hipótese nula.
